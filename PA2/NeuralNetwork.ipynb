{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pmczyC8mMQuY"},"outputs":[],"source":["import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import confusion_matrix"]},{"cell_type":"code","source":["def fit_NeuralNetwork(X_train,y_train,alpha,hidden_layer_sizes,epochs):\n","\n","    #Enter implementation here\n","    # Initialize the epoch errors\n","    err=np.zeros((epochs,1))\n","\n","    # Initialize the architecture\n","    N, d = X_train.shape\n","    X0 = np.ones((N,1))\n","    X_train = np.hstack((X0,X_train))\n","    d=d+1\n","    L = len(hidden_layer_sizes)\n","    L=L+2\n","\n","    #Initializing the weights for input layer\n","    weight_layer = np.random.normal(0, 0.1, (d,hidden_layer_sizes[0])) #np.ones((d,hidden_layer_sizes[0]))\n","    weights = []\n","    weights.append(weight_layer) #append(0.1*weight_layer)\n","\n","    #Initializing the weights for hidden layers\n","    for l in range(L-3):\n","        weight_layer = np.random.normal(0, 0.1, (hidden_layer_sizes[l]+1,hidden_layer_sizes[l+1]))\n","        weights.append(weight_layer)\n","\n","    #Initializing the weights for output layers\n","    weight_layer= np.random.normal(0, 0.1, (hidden_layer_sizes[l+1]+1,1))\n","    weights.append(weight_layer)\n","\n","    for e in range(epochs):\n","        choiceArray=np.arange(0, N)\n","        np.random.shuffle(choiceArray)\n","        errN=0\n","        for n in range(N):\n","            index=choiceArray[n]\n","            x=np.transpose(X_train[index])\n","            #TODO: Model Update: Forward Propagation, Backpropagation\n","            # update the weight and calculate the error\n","\n","            X,S = forwardPropagation(x,weights)\n","            g = backPropagation(X,y_train[index],S,weights)\n","            weights = updateWeights(weights, g, alpha)\n","            errN += errorPerSample(X, y_train[index])\n","\n","        err[e]=errN/N\n","    return err, weights"],"metadata":{"id":"PYnZ6AVtMbGh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def forwardPropagation(x, weights):\n","    #Enter implementation here\n","    l=len(weights)+1\n","    currX = x\n","    retS=[]\n","    retX=[]\n","    retX.append(currX)\n","\n","    # Forward Propagate for each layer\n","    for i in range(l-1):\n","\n","        currS= np.dot(currX, weights[i])\n","        retS.append(currS)\n","        currX=currS\n","        if i != len(weights)-1:\n","            for j in range(len(currS)):\n","                currX[j]= activation(currS[j])\n","            currX= np.hstack((1,currX))\n","        else:\n","            currX= outputf(currS)\n","        retX.append(currX)\n","    return retX,retS\n"],"metadata":{"id":"n3nQ2MOSMuNi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def errorPerSample(X,y_n):\n","    #Enter implementation here\n","    err = errorf(X[len(X)-1],y_n)\n","    return err"],"metadata":{"id":"qLiGHcb-MsLx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def backPropagation(X,y_n,s,weights):\n","    #Enter implementation here\n","    #x:0,1,...,L\n","    #S:1,...,L\n","    #weights: 1,...,L\n","    l=len(X)\n","    delL=[]\n","\n","    # To be able to complete this function, you need to understand this line below\n","    # In this line, we are computing the derivative of the Loss function w.r.t the\n","    # output layer (without activation). This is dL/dS[l-2]\n","    # By chain rule, dL/dS[l-2] = dL/dy * dy/dS[l-2] . Now dL/dy is the derivative Error and\n","    # dy/dS[l-2]  is the derivative output.\n","    delL.insert(0,derivativeError(X[l-1],y_n)*derivativeOutput(s[l-2]))\n","    curr=0\n","\n","    # Now, let's calculate dL/dS[l-2], dL/dS[l-3],...\n","    for i in range(len(X)-2, 0, -1): #L-1,...,0\n","        delNextLayer=delL[curr]\n","        WeightsNextLayer=weights[i]\n","        sCurrLayer=s[i-1]\n","\n","        #Init this to 0s vector\n","        delN=np.zeros((len(s[i-1]),1))\n","\n","        #Now we calculate the gradient backward\n","        #Remember: dL/dS[i] = dL/dS[i+1] * W(which W???) * activation\n","        for j in range(len(s[i-1])): #number of nodes in layer i - 1\n","            for k in range(len(s[i])): #number of nodes in layer i\n","                #TODO: calculate delta at node j\n","                delN[j]=delN[j]+ WeightsNextLayer[j][k] * delNextLayer[k] * derivativeActivation(sCurrLayer[j])\n","\n","        delL.insert(0,delN)\n","\n","    # We have all the deltas we need. Now, we need to find dL/dW.\n","    # It's very simple now, dL/dW = dL/dS * dS/dW = dL/dS * X\n","    g=[]\n","    for i in range(len(delL)):\n","        rows,cols=weights[i].shape\n","        gL=np.zeros((rows,cols))\n","        currX=X[i]\n","        currdelL=delL[i]\n","        for j in range(rows):\n","            for k in range(cols):\n","                #TODO: Calculate the gradient using currX and currdelL\n","                gL[j,k]= np.dot(currX[j],currdelL[k])# Fill in here\n","        g.append(gL)\n","    return g"],"metadata":{"id":"KYbnEhfmMpfX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def updateWeights(weights,g,alpha):\n","    #Enter implementation here\n","    nW=[]\n","    for i in range(len(weights)):\n","        rows, cols = weights[i].shape\n","        currWeight=weights[i]\n","        currG=g[i]\n","        for j in range(rows):\n","            for k in range(cols):\n","                #TODO: Gradient Descent Update\n","                currWeight[j,k] -= (alpha*currG[j,k])\n","        nW.append(currWeight)\n","    return nW"],"metadata":{"id":"xbQUHYzJMocO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def activation(s):\n","    if s > 0:\n","      return s\n","    else:\n","      return 0"],"metadata":{"id":"e82MUCU4Mngs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def derivativeActivation(s):\n","    if s > 0:\n","      return 1\n","    else:\n","      return 0\n"],"metadata":{"id":"CST-CIabMmv1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def outputf(s):\n","\n","    return 1 / (1 + np.exp(-s))"],"metadata":{"id":"tMeG6gN0MlX4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def derivativeOutput(s):\n","     return outputf(s)*(1-outputf(s))\n"],"metadata":{"id":"5_2dMiLUMkGX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def errorf(x_L,y):\n","  if y==1:\n","    return -1*(np.log(x_L))\n","  else:\n","    return -1*(np.log(1-x_L))\n"],"metadata":{"id":"VKADRHTpMjNp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def derivativeError(x_L,y):\n","  if y == 1:\n","    return -1 / (x_L)\n","  else:\n","    return 1/(1 - x_L)\n"],"metadata":{"id":"iIX7pMt-MhyT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pred(x_n,weights):\n","\n","  X,S = forwardPropagation(x_n, weights)\n","  if  X[len(X) - 1][0] < 0.5:\n","    return -1\n","  else:\n","    return 1"],"metadata":{"id":"FynkM1ZhMenj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def confMatrix(X_train,y_train,w):\n","  matrix = np.zeros((2,2))\n","  N = np.shape(X_train)[0]\n","\n","  x_1 = np.ones((N, 1))\n","  X_train = np.hstack((np.ones((N, 1)), X_train))\n","\n","  for i in range (N):\n","\n","    pred_label = pred(X_train[i], w)\n","\n","    if pred_label ==  y_train[i]:\n","      if pred_label == 1:\n","        #true postive\n","        matrix[1][1] += 1\n","      else:\n","        #true negative\n","         matrix[0][0] += 1\n","    else:\n","      if pred_label == 1:\n","        #true postive\n","        matrix[0][1] += 1\n","      else:\n","        #false negative\n","         matrix[1][0] += 1\n","  # matrix = matrix.astype(int)\n","  return matrix"],"metadata":{"id":"yLGfISXbMelC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plotErr(e,epochs):\n","  # x_axis = np.arange(epochs)\n","  # plt.plot(x_axis, e, color='r')\n","  # plt.plot(epochs, e, color='r')\n","  epochs = list(range(1, epochs + 1))\n","  plt.plot(epochs, e, color='r')\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Average error per epoch')\n","\n","  plt.title('Average Error vs Epochs')\n","  plt.show()\n"],"metadata":{"id":"l2Ad_HaaMeZl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_SciKit(X_train, X_test, Y_train, Y_test):\n","  model = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(30,10), random_state=1)\n","  model.fit(X_train, Y_train)\n","  Y_pred = model.predict(X_test)\n","\n","  return confusion_matrix(Y_test, Y_pred)\n"],"metadata":{"id":"tGdPSzwQMeQ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_Part1():\n","    from sklearn.datasets import load_iris\n","    X_train, y_train = load_iris(return_X_y=True)\n","    X_train, X_test, y_train, y_test = train_test_split(X_train[50:],y_train[50:],test_size=0.2, random_state=1)\n","\n","    for i in range(80):\n","        if y_train[i]==1:\n","            y_train[i]=-1\n","        else:\n","            y_train[i]=1\n","    for j in range(20):\n","        if y_test[j]==1:\n","            y_test[j]=-1\n","        else:\n","            y_test[j]=1\n","\n","    err,w=fit_NeuralNetwork(X_train,y_train,1e-2,[30, 10],100)\n","\n","    plotErr(err,100)\n","\n","    cM=confMatrix(X_test,y_test,w)\n","\n","    sciKit=test_SciKit(X_train, X_test, y_train, y_test)\n","\n","    print(\"Confusion Matrix is from Part 1a is: \",cM)\n","    print(\"Confusion Matrix from Part 1b is:\",sciKit)\n","\n","test_Part1()"],"metadata":{"id":"A8jTdL4AMdOW"},"execution_count":null,"outputs":[]}]}